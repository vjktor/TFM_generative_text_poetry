{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Generación de texto creativo con IA - Poesía - Transformer Decoder Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Victor\\anaconda3\\envs\\pt3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict, concatenate_datasets\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adquisición de conjunto de datos de poesía, limpieza y pre-procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el conjunto de datos\n",
    "dataset = load_dataset(\"linhd-postdata/poesias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se filtra el dataset para usar solo las poesías en español, combinando grupo de train y test para unificar todas las poesías y así poder construir el vocabulario final. A partir de esto se prepara el texto para construir el modelo de lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_es = dataset.filter(lambda example: example['language'] == 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = DatasetDict({\n",
    "    'merged': concatenate_datasets([dataset_es['train'], dataset_es['test']])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = merged_dataset['merged']['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se preparan los datos. Comienza con una lista vacía llamada `words`. Luego, itera sobre una lista de textos llamada `all_texts`, divide cada texto en palabras individuales y las agrega a la lista `words`. Después, cuenta la frecuencia de cada palabra en `words` y crea un vocabulario de todas las palabras únicas. Se asigna a cada palabra un índice único y se crean diccionarios para mapear entre palabras e índices, y viceversa. Se define una longitud de secuencia y se generan muestras de entrenamiento dividiendo el texto en secuencias de palabras de longitud fija, donde cada muestra consiste en una secuencia de palabras seguida por la siguiente palabra en el texto. Esto se realiza para el total de datos y posteriormente para cada particion `train` y `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for text in all_texts:\n",
    "    words.extend(text.split())\n",
    "word_counts = Counter(words)\n",
    "\n",
    "vocab = list(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "int_to_word = {i: word for word, i in word_to_int.items()}\n",
    "\n",
    "SEQUENCE_LENGTH = 64\n",
    "samples = [words[i:i+SEQUENCE_LENGTH+1] for i in range(len(words)-SEQUENCE_LENGTH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = dataset_es['train']['text']\n",
    "test_texts = dataset_es['test']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_train = []\n",
    "for text in train_texts:\n",
    "    words_train.extend(text.split())\n",
    "word_counts_train = Counter(words_train)\n",
    "\n",
    "SEQUENCE_LENGTH = 64\n",
    "samples_train = [words_train[i:i+SEQUENCE_LENGTH+1] for i in range(len(words_train)-SEQUENCE_LENGTH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test = []\n",
    "for text in test_texts:\n",
    "    words_test.extend(text.split())\n",
    "word_counts_test = Counter(words_test)\n",
    "\n",
    "SEQUENCE_LENGTH = 64\n",
    "samples_test = [words_test[i:i+SEQUENCE_LENGTH+1] for i in range(len(words_test)-SEQUENCE_LENGTH)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones y clases para entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define una clase TextDataset que crea un conjunto de datos personalizado para entrenar el modelo de lenguaje. Toma muestras de texto y un diccionario que mapea palabras a enteros. Devuelve pares de secuencias de palabras, una como entrada y la otra como objetivo, ambas representadas como tensores de PyTorch. Se usa con cada partición  de `train` y `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, samples, word_to_int):\n",
    "        self.samples = samples\n",
    "        self.word_to_int = word_to_int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        input_seq = torch.LongTensor([self.word_to_int[word] for word in sample[:-1]])\n",
    "        target_seq = torch.LongTensor([self.word_to_int[word] for word in sample[1:]])\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = TextDataset(samples_train, word_to_int)\n",
    "test_dataset = TextDataset(samples_test, word_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usan `data loaders`, los cuales son utilizados en el entrenamiento y evaluación del modelo de lenguaje, permitiendo iterar sobre los datos en lotes de manera eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se define una función llamada `generate_square_subsequent_mask` que genera una máscara cuadrada para una secuencia. La máscara se utiliza para ocultar posiciones futuras en la secuencia durante el entrenamiento de modelos de lenguaje como Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"\n",
    "    Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "    Unmasked positions are filled with float(0.0).\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define una clase llamada PositionalEncoding que implementa el módulo de codificación posicional en un modelo de Transformer.  Esta clase agrega información sobre la posición absoluta de los tokens a los embeddings de entrada en el modelo, lo que lo ayuda a entender la secuencia y su orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param max_len: Input length sequence.\n",
    "        :param d_model: Embedding dimension.\n",
    "        :param dropout: Dropout value (default=0.1)\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs of forward function\n",
    "        :param x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación la clase TextGen define un modelo de generación de texto basado en Transformer, la cual puede generar secuencias de palabras a partir de una entrada dada. Acá se van cambiando valores de dropout para manejo de regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGen(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads):\n",
    "        super(TextGen, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(max_len=SEQUENCE_LENGTH, d_model=embed_dim)\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=self.decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    # Positional encoding is required. Else the model does not learn.\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        \n",
    "        # Generate input sequence mask with shape (SEQUENCE_LENGTH, SEQUENCE_LENGTH)\n",
    "        input_mask = generate_square_subsequent_mask(x.size(1)).to(x.device)\n",
    "        \n",
    "        x = self.pos_encoder(emb)\n",
    "        x = self.decoder(x, memory=x, tgt_mask=input_mask, memory_mask=input_mask)\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se manejan diferentes hiperparámetros de cara al entrenamiento del modelo, donde despues de diferentes ejercicios se establecieron los que aparecene a continuación.\n",
    "De cara a manejar la complejidad del modelo se probaron diferentes valores, sobre todo mas bajos en embed_dim, num_layers y num_heads, pero con valores mas bajos los resultados eran inferiores a los obtenidos. Se tienen en total poco mas de 12 millones de parámetros a entrenar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Victor\\anaconda3\\envs\\pt3\\lib\\site-packages\\transformers\\utils\\hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TextGen(\n",
    "    vocab_size=vocab_size, \n",
    "    embed_dim=100,\n",
    "    num_layers=2,  \n",
    "    num_heads=2,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextGen(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (emb): Embedding(298041, 20)\n",
      "  (decoder_layer): TransformerDecoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
      "    )\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=20, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=20, bias=True)\n",
      "    (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.5, inplace=False)\n",
      "    (dropout2): Dropout(p=0.5, inplace=False)\n",
      "    (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=20, out_features=20, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=20, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=20, bias=True)\n",
      "        (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        (dropout3): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=20, out_features=298041, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "12,394,617 total parameters.\n",
      "12,394,617 training parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_dataloader, test_dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    steps_per_epoch = len(train_dataloader)\n",
    "    total_steps = epochs * steps_per_epoch\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_train_loss = 0\n",
    "        running_test_loss = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for input_seq, target_seq in train_dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            outputs = model(input_seq)\n",
    "            target_seq = target_seq.contiguous().view(-1)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            loss = criterion(outputs, target_seq)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.detach().cpu().numpy()\n",
    "        \n",
    "        # Calculate and print training loss and perplexity\n",
    "        epoch_train_loss = running_train_loss / steps_per_epoch\n",
    "        train_perplexity = math.exp(epoch_train_loss)\n",
    "        print(f\"Epoch {epoch + 1}, train loss: {epoch_train_loss:.3f}, train perplexity: {train_perplexity:.3f}\")\n",
    "        \n",
    "        # Testing phase\n",
    "        test_loss, test_perplexity = test(model, test_dataloader, criterion)\n",
    "        print(f\"Epoch {epoch + 1}, test loss: {test_loss:.3f}, test perplexity: {test_perplexity:.3f}\")\n",
    "\n",
    "def test(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            outputs = model(input_seq)\n",
    "            target_seq = target_seq.contiguous().view(-1)\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            loss = criterion(outputs, target_seq)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return avg_loss, perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 7.585, train perplexity: 1968.358\n",
      "Epoch 1, test loss: 7.575, test perplexity: 1949.663\n",
      "Epoch 2, train loss: 5.559, train perplexity: 259.454\n",
      "Epoch 2, test loss: 7.567, test perplexity: 1934.109\n",
      "Epoch 3, train loss: 5.379, train perplexity: 216.743\n",
      "Epoch 3, test loss: 7.600, test perplexity: 1998.170\n",
      "Epoch 4, train loss: 5.344, train perplexity: 209.390\n",
      "Epoch 4, test loss: 7.597, test perplexity: 1992.839\n",
      "Epoch 5, train loss: 5.323, train perplexity: 204.930\n",
      "Epoch 5, test loss: 7.558, test perplexity: 1916.722\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs, train_dataloader, test_dataloader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se guarda el modelo entrenado en el equipo local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"C:/Users/Victor/OneDrive/Documentos/MasterIA/09_TFM/desarrollo/modelosfinal/modelos_generadores/from_scratch.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga del modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model = TextGen(embed_dim=100,\n",
    "    num_layers=2, \n",
    "    num_heads=2,\n",
    "    vocab_size=vocab_size).to(device)\n",
    "the_model.load_state_dict(torch.load(\"C:/Users/Victor/OneDrive/Documentos/MasterIA/09_TFM/desarrollo/modelosfinal/modelos_generadores/from_scratch.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextGen(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (emb): Embedding(298041, 100)\n",
       "  (decoder_layer): TransformerDecoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.5, inplace=False)\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "    (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.5, inplace=False)\n",
       "        (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        (dropout3): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=100, out_features=298041, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia -  Función generadora de poesía"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función toma un texto como entrada y devuelve un tensor de enteros que representa la secuencia de palabras en el texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_int_vector(text):\n",
    "    words = text.split()\n",
    "    input_seq = torch.LongTensor([word_to_int[word] for word in words[-SEQUENCE_LENGTH:]]).unsqueeze(0)\n",
    "    return input_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función sample_next toma las predicciones de un modelo como entrada y devuelve el índice de la siguiente palabra a generar utilizando un enfoque de muestreo aleatorio, para que genere diferentes salidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next(predictions):\n",
    "    \"\"\"\n",
    "    Random sampling.\n",
    "    \"\"\"\n",
    "    # Random sampling approach.\n",
    "    probabilities = F.softmax(predictions[:, -1, :], dim=-1).cpu().numpy()\n",
    "    next_token = random.choices(range(len(probabilities[0])), weights=probabilities[0])[0]\n",
    "    return int(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función genera texto a partir del modelo de lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generativo_texto(sentence, generate_length=100):\n",
    "    texto_generado = sentence\n",
    "    the_model.eval()\n",
    "\n",
    "    for _ in range(generate_length):\n",
    "        int_vector = return_int_vector(texto_generado)\n",
    "        \n",
    "        if len(int_vector) >= SEQUENCE_LENGTH - 1:\n",
    "            break\n",
    "        \n",
    "        input_tensor = int_vector.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = the_model(input_tensor)\n",
    "        \n",
    "        next_token = sample_next(predictions)\n",
    "        texto_generado += ' ' + int_to_word[next_token]\n",
    "\n",
    "    return texto_generado\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interfaz web para generar poesía con Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la libreria Gradio, a partir de la función anterior, se genera una interfaz web que permite ingresar un texto inicial y así generar poesía a partir de dicho texto. La interfaz permite interactuar con ella en el presente notebook o mediante un link web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "interfaz = gr.Interface(\n",
    "    fn=generativo_texto,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Generador de Poesía\",\n",
    "    description=\"Introduce un texto inicial y genera poesía\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se lanza la interfaz web por medio de launch()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://1233ab91bf8d884a0b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1233ab91bf8d884a0b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interfaz.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cierra la interfaz web por medio de close()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "interfaz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Al ver las estrellas pienso en ti'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    " s = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Al', 'ver', 'las', 'estrellas', 'pienso', 'en', 'ti']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ojos asi como  si fueran las así, centinela. He visto desaparecer si he de estrechar juntos, este aire anciano... Déjame en la primavera que callada noche el nido entre labios y frío. No le hablé de piedad tienden suavemente al seductores, No es dable alguna vez que quiero?... Por eso lloro un olor dado que siempre es la costumbre de sentarnos y que vienen aquellas veces trajinada las bulle desvanecido, montada entonces los homosexuales y mandíbula; Días crecen las frentes de las niñas solamente; y las andaban por un solo aire de un río de donde verdea ámame por sus vidas con el\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    poesia = texto_generador(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "poesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Al', 'ver', 'las', 'estrellas', 'pienso', 'en', 'ti']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Al ver las estrellas pienso en ti'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_texto(sentence):\n",
    "    for sentence in sentences:\n",
    "        print(f\"PROMPT: {sentence}\")\n",
    "    return text_generator(sentence, generate_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_texto(sentence):\n",
    "    for sentence in sentences:\n",
    "    texto_generador(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Ojos asi como \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Ojos asi como \n",
      "Ojos asi como  por los últimos pisos, que ya va propicio que no tenga aureola ni oirás calva sangre con tus virotes la oscura en las que me metas en el fondo del desordenado el hueco de tu mirada, esta noche sin embargo, ya no es tanto como si piensa en el que hay sin tregua con tu puedo y con mi nombre y sobre mi vida perfecta! conocerte, así vengo hasta que raíz alcanzará el alma quisiera en ti. Feliz cumpleaños con el inmediato, milagro de tus crías, encima, en tus cerraduras a la sangre irán las furias vivos a éste, explorando\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(f\"PROMPT: {sentence}\")\n",
    "    text_generator(sentence, generate_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_texto(sentence):\n",
    "    for sentence in sentences:\n",
    "        print(f\"PROMPT: {sentence}\")\n",
    "    return text_generator(sentence, generate_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Ojos\n",
      "Ojos que su Jesucristo; Te sabía, mayor, antes que antes cebra divinas despedaza. Dejad que el comunicando tommy derk «Para mí» doble aparato de locura. ¿Querrás lo perdido Que os remeden. y tumbas anónima a la sombra servís del soneto Miran te expulsaron no largos nosotros no está... el alma casa, nos habrá tiempo será formal, de vuestro amado Nos valores y abogados en siglo en tierras lejanas, y sobre la oscura patria amarga condición, Él toda blanca, sin dudar mujer ni libro ni nombre. ¿Cómo expresarme ni don Dinero. Ya le rece su gemido? homicidas a robarse para tú el\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Ojos\n",
      "Ojos por los últimos pisos, porque el tuétano del bosque penetrará al jardín de un instante en una fragua del buey nos Tracia. Me guardan una mano una cinta Labrada con liebre cobarde y divina Otros, que hinche la azucena esbelta, Elsa motivos silba ahogado por las ilusiones, la cima irrupciones de carne sentada en altura de nadar, por la Muerte, hielos, el paisaje con la muerte. rruego; de rosa y una mujer y una estrella. Hoy hiciste desviste de risa, luego, una penumbra plena libertad de garza a fausto y perderlas los jazmines y en los ojos brillan de Neptuno,\n",
      "\n",
      "\n",
      "PROMPT: Ojos\n",
      "Ojos que sus estrellas radiantes miras. Pero tiemblen tan morenas pero quizá no casan impenetrables perderá? los ha de trabajar en pura Lo que la vida es dicha mas no es cosa que el verdugo inflame Que censure la primera Azagra suavemente un vivo en su Tenorio, la idea, para él. Y aun saber, que las alas Del frasco Con sus bocas de angustiado en leda suposición Y prados. Tú sobre todo lo debes contar tus ojos les (lo abandonaron juntos. Mi amargura y adolescentes rebeldes. Puedo decir una cultura es el cuadrado para mí me vemos para tu cuerpo, pero\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "interfaz = gr.Interface(\n",
    "    fn=gen_texto,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Generador de Poesía\",\n",
    "    description=\"Introduce un texto inicial y genera poesía\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://c4eb23da9f23aad73e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c4eb23da9f23aad73e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Ojos\n",
      "Ojos e syn peresa; como desides, don Melón para Papa —murmura vuestra memoria? ¿El despertaba quisimos prestar. Trayectorias semiparalelas, fim repartiendo la vida por una herradura la noche solitario y como una voz incapaz muerto, y envió cuando ahora fue juntado, hijo, debo El cortesano es algo». usted he visto entonces me huesos? Osa, mirad, tener una tarde para tener necesidad de contemplar la vida. Y en el sol. Viene a ese filo, crecer Pálida desnuda en boca traducido y mi salvación dormido. Entre la carita del páramo sombrío... Frío mi verte». Y un depositarios Verde sombrío, sin aliento, Todos soltaron\n",
      "\n",
      "\n",
      "PROMPT: Ojos\n",
      "Ojos cual los ¿Andas ¡cómo en las cuales pobladores del silencio residen áspides sus huecos el oro de mi raza se mueve para romper el índico nopal, espumas de la zona de las aguas. De Enano entre la noche rápida rizos tan Ejemplo de la playa ya confusa, de plata. Y lloro; y días, senos que se agota, sombra. También busco en piedras y de sol y luna: Natural ¡oh enebro. Y pasó bajo la maravilla lluvia que suenan sus profundas hojas Sigue de crines y son nubes, raudas saludos que Nihil hacia el nogal mala, de la Guerra de sus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interfaz.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "interfaz.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
