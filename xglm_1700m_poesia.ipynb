{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de texto creativo con IA - Poesía"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XGLMTokenizerFast, XGLMForCausalLM\n",
    "from transformers import DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from tokenizers import AddedToken\n",
    "from datasets import load_metric\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de modelo, tokenizador y ajuste de tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga la versión 1.7B del modelo XGLM, indicándole que el dispositivo que se usará es la GPU (CUDA: Compute Unified Device Architecture de Nvidia). Adicionalmente se le indica la ruta de la máquina local donde quedará alojado el modelo modelo descargado. Se agrega el token especial '\\n' que hace la función de salto de línea, el cual no está en el vocabulario del tokenizador. Adicionalmente se redimensiona el tamaño del token embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XGLMTokenizer.from_pretrained(\"facebook/xglm-1.7B\")\n",
    "model = XGLMForCausalLM.from_pretrained(\"facebook/xglm-1.7B\", device_map=\"cuda\", load_in_8bit=True, cache_dir= 'I:/transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    'additional_special_tokens': [AddedToken('\\n')]\n",
    "}\n",
    "\n",
    "# Modifica los tokens especiales del tokenizador\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adquisición de conjunto de datos de poesía, limpieza y pre-procesado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el conjunto de datos que contiene las poesías. Se eligen únicamente los textos en español y se pre-procesa el conjunto de datos, agregándole los tokens representarán el inicio y final de cada poesía. Despues, se tokeniza el conjunto de datos, truncandolos a 512 máximo de longitud y se realiza padding para rellenar aquellos textos que tenga una longitud menor. Finalmente se declara el data collator que se usará en el entrenamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el conjunto de datos\n",
    "dataset = load_dataset(\"linhd-postdata/poesias\")\n",
    "\n",
    "# Acceder a la información del conjunto de datos\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_spanish = dataset.filter(lambda example: example['language'] == 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ds(example):\n",
    "  example[\"text\"] = \"<s>\" + example['text'] + \"</s>\"\n",
    "  return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_spanish = dataset_spanish.map(format_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = dataset_spanish.map(\n",
    "    lambda example: tokenizer(example[\"text\"], max_length=512, truncation=True),\n",
    "    batched=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,  mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y evaluación por época"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se itera sobre los parámetros del modelo y los congela para que no se actualicen durante el entrenamiento y se observa la cantidad final de parámetros del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues se usa la biblioteca peft para configurar un modelo LoRA (Low-Rank Adaptation of Large Language Models) con una configuración específica definida por LoraConfig. Después, se imprime el número de parámetros entrenables y el número total de parámetros del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, # num de variabkle sindep a tener cada matriz para modificar comportamiento, se\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=['embed_tokens', 'lm_head']\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En training_args se definen los argumentos principales de entrenamiento del modelo y esto alimenta el trainer, que se usará para entrenar el modelo y realizar la evaluación por época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='I:/transformers/xglmmed3',\n",
    "    learning_rate= 0.0001, \n",
    "    save_steps = 1000000,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.001,\n",
    "    gradient_accumulation_steps=2,\n",
    "    do_eval = True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy  = 'epoch',\n",
    "    eval_steps = 1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardado del modelo y métricas de rendimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo entrenado y las métricas de rendimiento se guardan en el equipo local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained(\"C:/Users/Victor/OneDrive/Documentos/MasterIA/09_TFM/desarrollo/modelosfinal/modelos_generadores/xglm_1_7b_poesia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logh = trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = []\n",
    "\n",
    "# Itera sobre cada diccionario en la lista 'data'\n",
    "for entry in logh:\n",
    "    # Extrae las claves 'loss', 'step' y 'eval_loss' de cada diccionario\n",
    "    extracted_entry = {'loss': entry.get('loss', None),\n",
    "                       'step': entry.get('step', None),\n",
    "                       'eval_loss': entry.get('eval_loss', None)}\n",
    "\n",
    "    # Agrega el diccionario extraído a la nueva lista\n",
    "    extracted_data.append(extracted_entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un DataFrame de pandas con los datos extraídos\n",
    "df = pd.DataFrame(extracted_data)\n",
    "csv_file_path = 'C:/Users/Victor/OneDrive/Documentos/MasterIA/09_TFM/desarrollo/modelosfinal/metricas_modelo/loss_xglm_1_7b.csv'\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despliegue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de modelo, tokenizador y ajuste de tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el modelo almacenado, así como el tokenizador original que se vuelve a ajustar para incluir el caracter '\\n'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_modelo = \"C:/Users/Victor/OneDrive/Documentos/MasterIA/09_TFM/desarrollo/modelosfinal/modelos_generadores/xglm_1_7b_poesia\"\n",
    "# XGLMTokenizer, XGLMForCausalLM\n",
    "# Cargar el tokenizador y el modelo\n",
    "model = XGLMForCausalLM.from_pretrained(ruta_modelo, device_map=\"cuda\", load_in_8bit=True)\n",
    "tokenizer = XGLMTokenizerFast.from_pretrained(\"facebook/xglm-1.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    'additional_special_tokens': [AddedToken('\\n')]\n",
    "}\n",
    "\n",
    "# Modifica los tokens especiales del tokenizador\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función generadora de poesía"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del modelo cargado, se crea esta función generadora de texto, la cual se alimenta de una entrada textual y genera la poesía. Se declara el argumento do_sample a True para que genere diferentes poesías con la misma entrada. El argumento temperatura controla la aleatoriedad de la generación. Valores bajos producen texto mas conservador eligiendo palabras mas probables. Valores altos produce mayor variabilidad y creatividad en la generación. Un valor en 0.95 garantiza variabilidad y creatividad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_texto(Texto):\n",
    "    input_ids = tokenizer.encode(Texto, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    # Generar texto condicionalmente\n",
    "    output = model.generate(input_ids, max_length=250, min_length=100,pad_token_id=tokenizer.eos_token_id,\n",
    "                        #repetition_penalty = 1.2, num_beams=1, \n",
    "                        #num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95, \n",
    "                        temperature=0.95, do_sample=True,)\n",
    "\n",
    "# Decodificar y mostrar el texto generado\n",
    "    texto_generado = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "    \n",
    "    #texto_generado = \"Texto generado por el modelo...\" # Aquí deberías llamar a tu modelo generativo para generar texto\n",
    "    \n",
    "    return texto_generado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interfaz web para generar poesía con Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la libreria Gradio, a partir de la función anterior, se genera una interfaz web que permite ingresar un texto inicial y así generar poesía a partir de dicho texto. La interfaz permite interactuar con ella en el presente notebook o mediante un link web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interfaz = gr.Interface(\n",
    "    fn=generar_texto,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Generador de Poesía\",\n",
    "    description=\"Introduce un texto inicial y genera poesía\",\n",
    "    allow_flagging = 'never',\n",
    "    clear_btn = 'Nueva Poesía',\n",
    "    submit_btn = 'Crea Poesía'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se lanza la interfaz web por medio de launch()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interfaz.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cierra la interfaz web por medio de close()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interfaz.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
